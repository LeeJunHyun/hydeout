---
layout: post
title: '(작성중)[CS294] Lecture 2 - Supervised Learning and Imitation'
categories:
  - Deep Learning
tags:
  - deep learning
  - reinforcement learning
  - CS294
---

UC Berkeley의 강화학습 강의 CS294(2018 Fall semester)를 정리한 포스트입니다.

* [Course website](http://rail.eecs.berkeley.edu/deeprlcourse/)
* [Youtube video](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)
* [Reddit](https://www.reddit.com/r/berkeleydeeprlcourse/)


코스 전체에서 다루는 주제입니다. (자세한 항목은 syllabus 참조하시기 바랍니다.)
1. From supervised learning to decision making
2. Model-free algorithms: Q-learning, policy gradients, actor-critic
3. Advanced model learning and prediction
4. Exploration
5. Transfer and multi-task learning, meta-learning
6. Open problems, research talks, invited lectures

---
---
## 강의 내용
1. 연속적 의사결정(decision making)의 정의
2. 모방학습(imitation Learning): 의사결정을 위한 지도학습(supervised learning)
    * 직접모방이 잘 작동하는지?
    * 어떻게 더 잘하게 할건지?
3. (Deep) 모방학습의 최근 연구들
4. 모방학습에서 놓치고 있는 것들


* **목표**:
    * 정의과 표기법의 이해
    * 기초적인 모방학습에 대한 이해
    * 모방학습의 장단점
    
---
---

## 지도학습과 강화학습 용어 정리
![CS294-02-01](/assets/img/CS294/CS294-02-01.png)
기존 지도학습중에서 Classification task를 강화학습의 표기로 이어서 설명한다.
<p>매 시각 \( t \)마다 관측과 분류를 한다고 가정하면, 기존 이미지 input은 관측(observation) \( o_{t} \)로 표기되고, 동물의 종류를 구분하는 대신 agent의 행동(action)을 구분한다면 output은 \( a_{t} \)가 된다. observation \( o_{t} \)가 주어졌을 때 action \( a_{t} \)의 확률분포를 내뱉는 것은 policy \( \pi_\theta(a_{t}|o_{t}) \)이고, \( \theta \)는 뉴럴넷과 같은 파라미터가 된다. \(o_{t}\)는 state \(s_{t}\)로부터 나온다. </p>

![CS294-02-02](/assets/img/CS294/CS294-02-02.png)
<p>
\( o_{t} \)와 \( s_{t} \)의 관계를 좀 더 설명하면, 위 그림을 보면 영양이 치타에게 쫓기고 있는데, 이 사진은 픽셀로 표현된 관측 \( o_{t} \)이고, state \( s_{t} \)는 두 객체의 (역학적) 상태라고 볼 수 있다. 우리는 똑똑하게도 사진 \( o_{t} \)만 보고도 state \( s_{t} \)를 알 수 있지만 기계는 그렇지 못하다. (학습을 해야한다.) 이 때 누군가 운전해와서 자동차가 치타를 가리게되면 관측 \( o_{t} \)는 변하지만 치타와 영양의 상태 \( s_{t} \)는 변하지 않는다.
</p>

![CS294-02-03](/assets/img/CS294/CS294-02-03.png)
<p>
따라서 state \( s_{t} \), observation \( o_{t} \) 그리고 action \( a_{t} \) 의 관계를 그래프로 그려보면 위와같다. 그리고 state \( s_{t} \)에서 action \( a_{t} \)를 취했을때 \( s_{t+1} \)로 가는 확률을 \( p(s_{t+1}|s_{t},a_{t}) \)로 나타낸다. 이때 \( s_{t} \)에서 \( s_{t+1} \)로 갈때 \( s_{t-1} \)과는 독립적이다. 이를 Markov property라고 한다. 다시말해, 과거 상태 \( s_{t-1} \)와 현재 상태 \( s_{t} \)가 주어졌을 때의 미래 상태의 조건부 확률 분포 \( p(s_{t+1}|s_{t},a_{t}) \)가 과거 상태와는 독립적으로 현재 상태 \( s_{t} \)에 의해서만 결정된다는 것을 뜻한다.
</p>

간혹 어떤 논문에서는 state를 unknown variable 이라 x로 두고, 러시아어에서 앞글자를 따 action을 u라고 표기하기도 한다. (feat. Lev Pontryagin)

---
## Imitation Learning
![CS294-02-04](/assets/img/CS294/CS294-02-04.png)
<p>
운전하는 사람으로부터 observation \( o_{t} \)와 사람의 action \( a_{t} \)의 데이터를 쌓아서, \( \pi_\theta(a_{t}|o_{t}) \) 를 지도 학습하는 것을 behavior cloning이라고 한다.
<strong>일반적으로는 잘 안된다.</strong> training data에 없던 data가 test에 나온다던지, 애초에 학습데이터의 사람의 action이 bad action이었던지, 사람이 비슷한 상황 \( o_{t} \)에 대해 서로 다른 action \( a_{t} \)를 취한다던지, 여러 이유가 있을 수 있다.
</p>

[](){:name='drift'}
![CS294-02-05](/assets/img/CS294/CS294-02-05.png)
> distributional drift
<p>
그러나 본질적인 문제는 여기에 있다. 학습 알고리즘이 아무리 좋다해도 약간의 오차가 발생 할 수 있는데, 처음에는 매우 미세한 차이로 시작하더라도, 그 차이들이 계속 누적되면 결국엔 원래의 학습 데이터와 큰 차이로 나타나게 되는 것이다. 위 그림에서 검정색 선이 학습데이터의 경로 \( p_{data}(o_{t}) \)이고, 빨간 선이 학습된 경로 \( p_{\pi_{\theta}}(o_{t}) \)이다. 이때 우리는 <strong>어떻게 해야 \( p_{data}(o_{t}) = p_{\pi_{\theta}}(o_{t}) \)로 만들 수 있을까?</strong>
</p>

#### Random shift and Noise?
![CS294-02-06](/assets/img/CS294/CS294-02-06.png)
Bojarski et al. '16, NVIDIA



#### DAgger: Dataset Agrregation
[A Reduction of Imitation Learning and Structured Prediction
to No-Regret Online Learning](https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf), Ross et al. '11

목표 : 학습데이터를 $ p_{data}(o_{t}) $ 대신에 $ p_{\pi_{\theta}}(o_{t}) $에서 모으는 것이다.<br>
어떻게? 그냥 $ \pi_{\theta}(a_{t}|o_{t}) $를 돌린다.<br>
그러나 labels $ a_{t} $가 필요하다.<br>

1. 사람의 데이터 $ \mathcal{D} = \\{ o_{1},a_{1},...,o_{N},a_{N} \\}$ 을 모아서(사람이 직접 운전을 해서) $ \pi_{\theta}( a_{t} \| o_{t} ) $ 를 학습시킨다.
2. $ \pi_{\theta}(a_{t} \| o_{t}) $를 돌려서 (자율주행을 시켜서) $ \mathcal{D_{\pi}} = \\{ o_{1},...,o_{M} \\}$ 을 얻는다.
3. 사람이 $ \mathcal{D_{\pi}} $에 대해 action $ a_{t} $ label을 달아준다.
4. $ \mathcal{D} \leftarrow  \mathcal{D} \cup \mathcal{D_{\pi}} $ 합쳐준다.
5. 1~4를 반복한다.

* 문제점 : 사람의 노동력이 너무 많이 들어간다.

**어떻게 해야 더 많은 데이터 없이 성능을 올릴 수 있을까?**

DAgger은 [distributional drift](#drift) 문제를 해결하였다. 

만약 우리 모델이 전문가를 정확하게 모방하되 overfit 하지 않는다면 성능을 올릴 수 있을 것이다. 그렇다면 왜 전문가를 모방하는데 실패하였는가?

1. Non-Markovian behavior:
    * Markov property를 이용한다면 과거의 상태에 독립적으로 현재상태에서 최적의 선택을 해야하는데, 사람은 그렇지 못하다. 즉, Markovian behavior인 $ \pi_{\theta}(a_{t} \| o_{t}) $에서는 현재 상태에만 의존해서 행동해야하는데, 실제로는 지난 관측들까지 고려하는 $ \pi_{\theta}(a_{t} \| o_{1},...,o_{t}) $인 것이다.
    * **개선:** 지난 상태들(history)을 반영하기위해 RNN(Recurrent Neural Network)을 사용한다. 그 중 LSTM이 주로 사용된다.
    ![CS294-02-07](/assets/img/CS294/CS294-02-07.png)

2. Multimodal behavior:
    * 사람은 가끔 왼손으로 운전했다가, 오른손으로 운전했다가, 슬펐다가 기뻤다가 등등 데이터에 영향을 주는 많은 (예측할 수 없는) 요인을 갖고 있다. 따라서 같은 상황에서도 다른 선택을 한다.
    * **개선:** 
        1. Output Mixture of Gaussians : 최종 행동을 뽑을때 하나의 Gaussian에서 뽑는 것이 아니라 여러개의 Gaussian을 합쳐서 뽑는다. 단순한 방법이다.
        
        
        2. Latent variable model : 일반적으로 사용되긴 하지만 다소 복잡하다.
        
        3. Autoregressive discretization : setup이 약간 지저분하지만 Latent variable model보다는 단순하다.

![CS294-02-08](/assets/img/CS294/CS294-02-08.png)
















