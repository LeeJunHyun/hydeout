---
layout: post
title: '(작성중)[Paper] Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation'
categories:
  - Deep Learning
tags:
  - deep learning
  - geometric deep learning
  - reinforcement learning
  - generative adversarial network
  - drug discovery
  - paper review
excerpt_separator: <!--more-->
---

이 논문을 정리한 포스트입니다.

[Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation](https://arxiv.org/pdf/1806.02473.pdf), Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec, NIPS 2018
<!--more-->

* Abstract
* 1 Introduction
    * Present Work
    * Graph representation
    * Reinforcement learning
    * Adversarial training

* 2 Related Work

* 3 Proposed Method
    * 3.1 Problem Definition
    * 3.2 Graph Generation as Markov Decision Process
    * 3.3 Molecule Generation Environment
        * State Space
        * Action Space
        * State Transition Dynamics
        * Reward design
    * 3.4 Graph Convolutional Policy Network
        * Computing node embeddings
        * Action prediction
    * 3.5 Policy Gradient Training

* 4 Experiments
    * Property Optimization
    * Property Targeting
    * Constrained Property Optimization
    * 4.1 Experimental Setup
        * Dataset
        * Molecule environment
        * GCPN Setup
        * Baselines

    * 4.2 Molecule Generation Results
        * Property optimization
        * Property Targeting
        * Constrained Property Optimization

* 5 Conclusion

---
---

## Abstract
기초적인 규칙을 지키면서도 새로운 그래프 구조를 만들어 내는 것은 화학, 생물학 등에서 중요하다. 특히, 이 논문처럼 그럴듯하면서도 새로운 분자 그래프를 만들어 내는 것은 신약개발에서 더욱 중요하다. 그럴듯 하다는것은 drug-likeness를 가져야 할 뿐 아니라, chemical valency같은 물리적인 규칙도 만족시켜야 한다는 것을 의미한다. 그러나 원하는 특성을 가진 분자를 찾는 모델을 만드는 것은 굉장히 복잡하고 미분불가능한 규칙때문에 어려웠다. 본 논문에서는 Graph Convolutional Policy Network(GCPN)을 제안한다. Graph convolutional network와 강화학습(policy gradient)을 이용하여 목적에 맞게 그래프를 만들어준다. reward를 domain에 맞게 주고, policy gradient를 통해 adversarial loss를 준다. 최적화 성능이 state-of-the-art보다 chemical property에서 61%, constrained property에서 184% 올랐다고 한다.

## 1 Introduction
신약개발이나 재료과학에서 원하는 특성을 갖는 새로운 분자구조를 설계하는 것이 중요하다. 그러나, 탐색해야할 chemical space가 어마어마하게 크기때문에 굉장히 어려웠다. 게다가 chemical space는 불연속적이고 작은 변화에도 분자 특성이 크게 변한다.  

최근 딥러닝을 적용시켜 많은 발전이 있긴 했지만, 새로운 약물 구조를 만들어 물리적, 화학적, 생물학적 특성을 고루 만족시키는 것은 여전히 어려운 일이다. 또한 데이터셋에서 존재하는 분자의 분포랑 우리가 원하는 특성을 갖는 분자의 분포랑 다르기 때문에 결국 막대한 chemical space를 탐색해야 한다.

* **Present Work**:
    여기서는 chemical rule을 지키도록 제한을 두면서 새로운 분자구조를 만들어내는 Graph Convolutional Policy Network (GCPN)를 제안한다. 크게 세가지 아이디어를 하나의 프레임워크로 묶어서 사용했는데, 이어서 설명할 **graph representation**, **reinforcement learning**, **adversarial training**이다.

* **Graph representation**:
    그래프 구조의 vector repersentation을 만들기 위해 사용된다. 기존의 simplifed molecular-input line-entry system(SMILES)나 텍스트 기반 임베딩 보다 robust 하다. 예를 들면, text-based representation에서는 글자 하나만 바뀌어도 엄청난 변화가 생겨서 쓸모없게 되는 경우가 있다. 게다가, 생성된 분자의 일부분도 substructure로써 의미를 갖고 해석되는 경우가 있는데, text representation는 일부분만 봐서는 대개 의미가 없다. 그러나 그래프에서는 부분적으로 생성된 분자에 대해서도 valency check같은 chemical check를 할 수 있다.

    ![GCPN-01](/assets/img/Paper/GCPN/GCPN-01.png)
    *text based representation (seq-to-seq model) https://arxiv.org/pdf/1706.01643.pdf*

* **Reinforcement learning**:
    Reinforcement learning(RL)을 이용하는 것은 generative model을 사용하는 것보다 몇가지 이점이 있다.

    1. drug-likeness나 valency 같은, 제한적인 분자의 특성을 만족시켜야 한다. 그런데 이런 특성인 복잡하고 non-differentiable 하기 때문에, 하나의 generative network의 objective function으로 묶어서 학습시킬 수 없다. 반면에 강화학습은 environment dynamics와 reward function의 설계에 따라 hard constraint와 desired property들을 표현할 수 있다.
    2. 기존 generative model들은 학습 데이터셋의 범위 안에서 분자 그래프를 만들어 내는 반면에, 강화학습은 데이터셋 이외의 영역도 탐색한다(exploration).

* **Adversarial training**:
    데이터셋에 있는 분자에 대한 사전지식을 반영하는 것은 분자 생성에 매우 중요하다. 예를 들어, 약물은 보통 생리학적으로 안정적이고 비독성이다. 하나의 특성을 위해서 rule-based로 일일이 설정해주는 것은 가능하긴 하지만, 여러가지 특성의 조합을 위한 작업은 매우 어렵다. Adversarial loss를 통해 discriminator을 학습시킴으로써 데이터셋의 정보를 implicitly 학습하고 generator의 학습도 가이드한다. Discriminator는 graph convolutional network 구조로 이루어져있다. ([Semi-supervised classification with graph convolutional networks](https://arxiv.org/abs/1609.02907), [Convolutional networks on graphs for learning molecular fingerprints](https://hips.seas.harvard.edu/files/duvenaud-graphs-nips-2015.pdf))

GCPN은 chemistry-aware graph generation environment에서 학습하는 RL agent이다. 새로운 substructure이나 atom을 기존의 분자 그래프에 이어주거나, 기존의 atom들끼리 이어주는 방식으로 분자그래프를 생성한다. 

molecule property optimization, property targeting 그리고 conditional property
optimization 이 세가지의 테스크에 대해 실험한다. ZINC dataset을 사용하였다. 

## 2 Related Work
* Yang et al. [40] and Olivecrona et al. [30] : 각각 Monte Carlo tree search와 policy gradient를 이용해서 최적화한 RNN을 사용해서 SMILES string을 생성한다.

* Guimaraes et al. [26] and Sanchez-Lengeling et al. [33] : 학습 데이터셋의 분자와 비슷하도록 adversarial loss를 reinforcement learning의 reward에 적용시켰다.
![GCPN-02](/assets/img/Paper/GCPN/GCPN-02.png)


위 논문들에서는 text-based representation을 사용했는데, 본 논문에서는 graph representation을 사용함으로써 위에서 언급한 이점들을 취했다.

* [Jin et al.](https://arxiv.org/abs/1802.04364) : Variational autoencoder(VAE)을 사용했다. 분자구조를 atome들의 작은 클러스터들로 이루어진 junction tree로 나타냈다.
![GCPN-03](/assets/img/Paper/GCPN/GCPN-03.png)



















